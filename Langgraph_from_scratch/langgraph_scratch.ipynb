{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" First Function \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"to Second Function\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('node_1', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am moving from First Function to Second Function'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke('I am moving from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'node_1':\n",
      "---\n",
      "I am moving from First Function \n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'node_2':\n",
      "---\n",
      "I am moving from First Function to Second Function\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'I am moving from'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating LLM call in the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_time': 0.028912266, 'prompt_time': 0.003753027, 'queue_time': 0.029514092999999998, 'total_time': 0.032665293}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c5e948a-ffbc-4bf7-a993-a16108c7771f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model_name=\"llama3-groq-70b-8192-tool-use-preview\")\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the topic based on the user query. \\\n",
    "        Only output the topic among: [Japan , Sports]. Don't include reasoning. Following is the user query: \" + input_1\n",
    "    response = llm.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    TOPIC_UPPER = input_2.upper()\n",
    "    response = f\"Here is the topic in UPPER case: {TOPIC_UPPER}\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"Agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge('Agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"Agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the topic in UPPER case: JAPAN'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about Japan's Industrial Growth\"\n",
    "app.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'Agent':\n",
      "---\n",
      "Japan\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "Here is the topic in UPPER case: JAPAN\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(query):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "### Reading the txt files from source directory\n",
    "\n",
    "loader = DirectoryLoader('../data', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\LangGraph-End-to-End-Course\\venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\sunny\\LangGraph-End-to-End-Course\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sunny\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "###  BGE Embddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Retriever using Vector DB\n",
    "\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April'), Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='in select regions, and a standalone website. Both services use a Llama 3 model.[12] Reception was'), Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='version of Llama 3 as being \"surprisingly capable\" given it\\'s size.[11]'), Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunny\\AppData\\Local\\Temp\\ipykernel_21444\\2266497744.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about llama3?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[-1]   ## Fetching the user question\n",
    "    \n",
    "    complete_query = \"Your task is to provide only the topic based on the user query. \\\n",
    "        Only output the topic among: [Japan , Sports]. Don't include reasoning. Following is the user query: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending LLM call response to the AgentState\n",
    "    return state\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"Agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge('Agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"Agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Llama 3 model is a state-of-the-art AI model developed by Meta AI. It was first released in February 2023 and the latest version was released in April. The model has been tested to be capable of performing tasks similar to other top models such as PaLM and Chinchilla. It has been described as \"surprisingly capable\" given its size. The model is used in various services, including a chatbot and a standalone website, and has received positive reception in select regions.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"Tell me about llama3 model\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Llama 3 is \"surprisingly capable\" given its size.\\n2. Llama 3 is the latest version, released in April.\\n3. Llama 3 is on par with state of the art models such as PaLM and Chinchilla.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"tell me 3 property of llama3?\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'Agent':\n",
      "---\n",
      "{'messages': ['tell me 3 property of llama3?', 'Japan', 'Japan']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "1. Llama 3 is \"surprisingly capable\" given its size.\n",
      "2. It is the latest version of Llama, released in April.\n",
      "3. It is considered to be one of the state of the art models, such as PaLM and Chinchilla.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplyfying the State addition and maintainence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel , Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description='Selected Topic')\n",
    "    #Reasoning: str = Field(description='Reasoning behind topic selection')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    print('-> Calling Agent ->')\n",
    "    messages = state['messages']\n",
    "    question = messages[-1]   ## Fetching the user question\n",
    "    \n",
    "    templete = \"\"\" Your task is to provide only the topic based on the user query. \n",
    "        Only output the topic among: [Japan , Sports , Not Related]. Don't include reasoning. Following is the user query:  {question}\n",
    "        {format_instructions} \"\"\"\n",
    "    prompt = PromptTemplate(template=templete,\n",
    "                                    input_variables=[question],\n",
    "                                    partial_variables={\n",
    "                                        \"format_instructions\" : parser.get_format_instructions()                                    }\n",
    "                                    )\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    response = chain.invoke({\"question\":question,\"format_instructions\" : parser.get_format_instructions() })\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}\n",
    "\n",
    "def function_2(state):\n",
    "    print('-> Calling RAG ->')\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}\n",
    "\n",
    "def function_3(state):\n",
    "    print('-> Calling LLM ->')\n",
    "\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}\n",
    "\n",
    "\n",
    "def router(state):\n",
    "    print('-> Router ->')\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    if 'Japan' in last_message or 'Sports' in last_message:\n",
    "        return 'RAG Call'\n",
    "    else:\n",
    "        return 'LLM Call'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "\n",
    "graph = StateGraph(AgentState) ### StateGraph with AgentState\n",
    "\n",
    "graph.add_node(\"agent\", function_1)\n",
    "graph.add_node(\"RAG\", function_2)\n",
    "graph.add_node(\"LLM\", function_3)\n",
    "\n",
    "graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional edges are controlled by our router\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",  # where in graph to start\n",
    "    router,  # function to determine which node is called\n",
    "    {\n",
    "        'RAG Call': \"RAG\",\n",
    "        'LLM Call': \"LLM\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_edge(\"RAG\", END)\n",
    "graph.add_edge(\"LLM\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Calling Agent ->\n",
      "Topic='Japan'\n",
      "-> Router ->\n",
      "Japan\n",
      "-> Calling RAG ->\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"Tell me about Japan's Industrial Growth\"]}\n",
    "out = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Tell me about Japan's Industrial Growth\",\n",
       " 'Japan',\n",
       " \"There is no information about Japan's Industrial Growth in the provided context.\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There is no information about Japan's Industrial Growth in the provided context.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Calling Agent ->\n",
      "Topic='Sports'\n",
      "-> Router ->\n",
      "Sports\n",
      "-> Calling RAG ->\n",
      "{'messages': ['Who came fourth for Ireland at the outdoor European Running Championships in 1998?', 'Sports', \"I'm sorry but I do not have the answer to that question.\"]}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"Who came fourth for Ireland at the outdoor European Running Championships in 1998?\"]}\n",
    "out = app.invoke(inputs)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Calling Agent ->\n",
      "Topic='Not Related'\n",
      "-> Router ->\n",
      "Not Related\n",
      "-> Calling LLM ->\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"Tell me about Bert Model\"]}\n",
    "out = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell me about Bert Model', 'Not Related', \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in 2018. It's designed to understand natural language processing (NLP) tasks and has been fine-tuned to perform well on a wide range of NLP tasks such as question answering, sentiment analysis, and text classification.\\n\\nBERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in the input sequence. These representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\\n\\nBERT has several key features that make it effective:\\n\\n1. **Contextual Understanding**: BERT is trained to understand the context in which a word is used, which is crucial for accurately modeling language. It does this by looking at the entire input sequence, rather than just the immediate context of a word.\\n\\n2. **Pre-training**: BERT is pre-trained on a large corpus of text, such as the entire Wikipedia and BookCorpus. This pre-training task involves predicting randomly masked words in the input sequence, as well as predicting whether two sentences are adjacent in the original text. This pre-training helps BERT learn a general understanding of language that can be fine-tuned for specific NLP tasks.\\n\\n3. **Fine-tuning**: BERT can be fine-tuned for specific NLP tasks by adding an additional output layer on top of the pre-trained model. This fine-tuning process involves adjusting the model's parameters to fit the specific task at hand.\\n\\n4. **State-of-the-art Results**: BERT has achieved state-of-the-art results on a wide range of NLP tasks, including the Stanford Question Answering Dataset (SQuAD), the GLUE benchmark, and many others.\\n\\n5. **Open-source**: BERT is an open-source model, which means that anyone can use it and modify it for their own purposes. This has led to a wide range of variants and extensions of BERT, each designed for specific use cases.\\n\\nOverall, BERT represents a significant advancement in NLP research and has the potential to greatly improve the accuracy and efficiency of many NLP applications.\"]\n"
     ]
    }
   ],
   "source": [
    "print(out['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in 2018. It's designed to understand natural language processing (NLP) tasks and has been fine-tuned to perform well on a wide range of NLP tasks such as question answering, sentiment analysis, and text classification.\n",
      "\n",
      "BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in the input sequence. These representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\n",
      "\n",
      "BERT has several key features that make it effective:\n",
      "\n",
      "1. **Contextual Understanding**: BERT is trained to understand the context in which a word is used, which is crucial for accurately modeling language. It does this by looking at the entire input sequence, rather than just the immediate context of a word.\n",
      "\n",
      "2. **Pre-training**: BERT is pre-trained on a large corpus of text, such as the entire Wikipedia and BookCorpus. This pre-training task involves predicting randomly masked words in the input sequence, as well as predicting whether two sentences are adjacent in the original text. This pre-training helps BERT learn a general understanding of language that can be fine-tuned for specific NLP tasks.\n",
      "\n",
      "3. **Fine-tuning**: BERT can be fine-tuned for specific NLP tasks by adding an additional output layer on top of the pre-trained model. This fine-tuning process involves adjusting the model's parameters to fit the specific task at hand.\n",
      "\n",
      "4. **State-of-the-art Results**: BERT has achieved state-of-the-art results on a wide range of NLP tasks, including the Stanford Question Answering Dataset (SQuAD), the GLUE benchmark, and many others.\n",
      "\n",
      "5. **Open-source**: BERT is an open-source model, which means that anyone can use it and modify it for their own purposes. This has led to a wide range of variants and extensions of BERT, each designed for specific use cases.\n",
      "\n",
      "Overall, BERT represents a significant advancement in NLP research and has the potential to greatly improve the accuracy and efficiency of many NLP applications.\n"
     ]
    }
   ],
   "source": [
    "print(out['messages'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = llm.bind(tools=[convert_to_openai_tool(multiply)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_with_tools.invoke('What is 35 * 46?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kh86', 'function': {'arguments': '{\"first_number\": 35, \"second_number\": 46}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 209, 'total_tokens': 242, 'completion_time': 0.102648621, 'prompt_time': 0.016858735, 'queue_time': 0.005984724, 'total_time': 0.119507356}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-93c696b1-af2b-4532-aa99-79f59934755b-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 35, 'second_number': 46}, 'id': 'call_kh86', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 33, 'total_tokens': 242})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_kh86',\n",
       "  'function': {'arguments': '{\"first_number\": 35, \"second_number\": 46}',\n",
       "   'name': 'multiply'},\n",
       "  'type': 'function'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls = response.additional_kwargs.get('tool_calls')\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: multiply\n",
      "Function Arguments: {\"first_number\": 35, \"second_number\": 46}\n",
      "{'id': 'call_kh86', 'function': {'arguments': '{\"first_number\": 35, \"second_number\": 46}', 'name': 'multiply'}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    print('Function Name:',tool_call.get('function').get('name'))\n",
    "    print('Function Arguments:',tool_call.get('function').get('arguments'))\n",
    "    print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "\n",
    "graph = StateGraph(AgentState) ### StateGraph with AgentState\n",
    "\n",
    "\n",
    "def invoke_model(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[-1]   ## Fetching the user question\n",
    "    return {\"messages\":[model_with_tools.invoke(question)]}\n",
    "\n",
    "\n",
    "graph.add_node(\"agent\", invoke_model)\n",
    "\n",
    "def invoke_tool(state):\n",
    "    tool_calls = state['messages'][-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    multiply_call = None\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call.get(\"function\").get(\"name\") == \"multiply\":\n",
    "            multiply_call = tool_call\n",
    "\n",
    "    if multiply_call is None:\n",
    "        raise Exception(\"No adder input found.\")\n",
    "\n",
    "    res = multiply.invoke(\n",
    "        json.loads(multiply_call.get(\"function\").get(\"arguments\"))\n",
    "    )\n",
    "\n",
    "    return {\"messages\" : [res]\n",
    "    }\n",
    "\n",
    "graph.add_node(\"tool\", invoke_tool)\n",
    "\n",
    "graph.add_edge(\"tool\", END)\n",
    "\n",
    "graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    tool_calls = state['messages'][-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls):\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "graph.add_conditional_edges(\"agent\", router, {\n",
    "    \"multiply\": \"tool\",\n",
    "    \"end\": END,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = graph.compile()\n",
    "\n",
    "output = app.invoke({\"messages\": [\"What is 123 * 456?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['What is 123 * 456?',\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_j9nd', 'function': {'arguments': '{\"first_number\": 123, \"second_number\": 456}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 209, 'total_tokens': 242, 'completion_time': 0.10267339, 'prompt_time': 0.016128917, 'queue_time': 0.006846070999999999, 'total_time': 0.118802307}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2272cb4b-07d0-44c6-a9b8-e4a82fed8ecf-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 123, 'second_number': 456}, 'id': 'call_j9nd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 33, 'total_tokens': 242}),\n",
       "  56088]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56088"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = app.invoke({\"messages\": [\"What is LLM?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['What is LLM?',\n",
       "  AIMessage(content='LLM can stand for several things, depending on the context:\\n\\n1. Large Language Model: LLM is a term used in the field of artificial intelligence, particularly in natural language processing. It refers to a language model that is trained on a large dataset and has a large number of parameters, allowing it to understand and generate complex sentences and paragraphs.\\n\\n2. Master of Laws: LLM is also an abbreviation for the Master of Laws degree, which is a postgraduate law degree that is designed for students who have already completed a first degree in law or a related field. It is typically a one-year full-time or two-year part-time program that allows students to specialize in a particular area of law.\\n\\n3. LLM can also stand for other things, such as \"Live Life to the Max\" or \"Love Life More,\" but these are less common uses of the abbreviation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 206, 'total_tokens': 384, 'completion_time': 0.567325958, 'prompt_time': 0.017067205, 'queue_time': 0.005854583, 'total_time': 0.584393163}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'stop', 'logprobs': None}, id='run-7027ca46-0093-4c91-9c81-e86af66bbe03-0', usage_metadata={'input_tokens': 206, 'output_tokens': 178, 'total_tokens': 384})]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LLM can stand for several things, depending on the context:\\n\\n1. Large Language Model: LLM is a term used in the field of artificial intelligence, particularly in natural language processing. It refers to a language model that is trained on a large dataset and has a large number of parameters, allowing it to understand and generate complex sentences and paragraphs.\\n\\n2. Master of Laws: LLM is also an abbreviation for the Master of Laws degree, which is a postgraduate law degree that is designed for students who have already completed a first degree in law or a related field. It is typically a one-year full-time or two-year part-time program that allows students to specialize in a particular area of law.\\n\\n3. LLM can also stand for other things, such as \"Live Life to the Max\" or \"Love Life More,\" but these are less common uses of the abbreviation.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 206, 'total_tokens': 384, 'completion_time': 0.567325958, 'prompt_time': 0.017067205, 'queue_time': 0.005854583, 'total_time': 0.584393163}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'stop', 'logprobs': None} id='run-7027ca46-0093-4c91-9c81-e86af66bbe03-0' usage_metadata={'input_tokens': 206, 'output_tokens': 178, 'total_tokens': 384}\n"
     ]
    }
   ],
   "source": [
    "print(output['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
